import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from collections import Counter
import re
from datetime import datetime, timedelta
import io
import base64
from typing import List, Dict, Tuple, Optional
import warnings
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import json
import networkx as nx
import matplotlib.pyplot as plt
from PIL import Image as PILImage
import tempfile
import os
warnings.filterwarnings('ignore')

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSSã‚¹ã‚¿ã‚¤ãƒ«
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    font-weight: bold;
    color: #1E3A8A;
    text-align: center;
    margin-bottom: 2rem;
}
.sub-header {
    font-size: 1.5rem;
    font-weight: bold;
    color: #3B82F6;
    margin: 1.5rem 0 1rem 0;
}
.metric-container {
    background-color: #F8FAFC;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #3B82F6;
    margin: 1rem 0;
}
.insight-box {
    background-color: #FEF3C7;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #F59E0B;
    margin: 1rem 0;
}
.error-box {
    background-color: #FEE2E2;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #EF4444;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

class SurveyAnalyzer:
    """ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.raw_data = None
        self.data = None
        self.processed_data = None
        self.embeddings = None
        self.clusters = None
        self.cluster_labels = None
        self.wordcloud_cache = {}
        self.network_graph = None
        self.trend_analysis = None
        self.column_mapping = {
            'respondent_id': None,
            'response_date': None,  
            'text_response': None
        }
        
    def validate_data(self, df: pd.DataFrame, respondent_col: str, date_col: str, text_col: str) -> Tuple[bool, List[str]]:
        """ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        errors = []
        
        # é¸æŠã•ã‚ŒãŸã‚«ãƒ©ãƒ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        selected_columns = [respondent_col, date_col, text_col]
        missing_cols = [col for col in selected_columns if col not in df.columns]
        if missing_cols:
            errors.append(f"é¸æŠã•ã‚ŒãŸã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {', '.join(missing_cols)}")
            return False, errors
        
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        if not pd.api.types.is_datetime64_any_dtype(df[date_col]):
            try:
                df[date_col] = pd.to_datetime(df[date_col])
            except:
                errors.append(f"'{date_col}'ã‚«ãƒ©ãƒ ãŒæ—¥ä»˜å½¢å¼ã«å¤‰æ›ã§ãã¾ã›ã‚“")
        
        # ç©ºå€¤ãƒã‚§ãƒƒã‚¯
        empty_responses = df[text_col].isna().sum()
        if empty_responses > 0:
            errors.append(f"'{text_col}'ã«ç©ºå€¤ãŒ{empty_responses}ä»¶ã‚ã‚Šã¾ã™ï¼ˆåˆ†ææ™‚ã«é™¤å¤–ã•ã‚Œã¾ã™ï¼‰")
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆè­¦å‘Šãƒ¬ãƒ™ãƒ«ï¼‰
        duplicate_ids = df[respondent_col].duplicated().sum()
        if duplicate_ids > 0:
            errors.append(f"'{respondent_col}'ã«é‡è¤‡ãŒ{duplicate_ids}ä»¶ã‚ã‚Šã¾ã™ï¼ˆæ³¨æ„ãŒå¿…è¦ã§ã™ï¼‰")
        
        return len(errors) == 0, errors
    
    def prepare_data(self, df: pd.DataFrame, respondent_col: str, date_col: str, text_col: str) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æç”¨ã«æº–å‚™"""
        # ã‚«ãƒ©ãƒ åã‚’æ¨™æº–åŒ–
        prepared_df = df.copy()
        prepared_df = prepared_df.rename(columns={
            respondent_col: 'respondent_id',
            date_col: 'response_date',
            text_col: 'text_response'
        })
        
        # æ—¥ä»˜ã®å¤‰æ›
        if not pd.api.types.is_datetime64_any_dtype(prepared_df['response_date']):
            prepared_df['response_date'] = pd.to_datetime(prepared_df['response_date'], errors='coerce')
        
        # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆå›ç­”ã‚’é™¤å¤–
        prepared_df = prepared_df.dropna(subset=['text_response'])
        prepared_df = prepared_df[prepared_df['text_response'].str.strip() != '']
        
        return prepared_df
    
    def preprocess_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
        if pd.isna(text):
            return ""
        
        # æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
        text = re.sub(r'\s+', ' ', str(text))
        text = text.strip()
        
        return text
    
    def extract_keywords(self, texts: List[str], min_length: int = 2) -> Counter:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        all_words = []
        
        for text in texts:
            if not text:
                continue
            
            # å˜ç´”ãªå˜èªåˆ†å‰²ï¼ˆæ—¥æœ¬èªå¯¾å¿œã®ãŸã‚æ”¹è‰¯ãŒå¿…è¦ï¼‰
            words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ a-zA-Z0-9]+', text)
            words = [w for w in words if len(w) >= min_length]
            all_words.extend(words)
        
        return Counter(all_words)
    
    def extract_ngrams(self, texts: List[str], n: int = 2) -> Counter:
        """N-gramæŠ½å‡º"""
        ngrams = []
        
        for text in texts:
            if not text or len(text) < n:
                continue
            
            # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®N-gram
            for i in range(len(text) - n + 1):
                ngram = text[i:i+n]
                if re.match(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]{2,}', ngram):  # æ—¥æœ¬èªã®ã¿
                    ngrams.append(ngram)
        
        return Counter(ngrams)
    
    def search_responses(self, texts: pd.Series, query: str, use_regex: bool = False) -> pd.Series:
        """ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢"""
        if use_regex:
            try:
                return texts.str.contains(query, case=False, regex=True, na=False)
            except:
                st.error("æ­£è¦è¡¨ç¾ã«ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Šã¾ã™")
                return pd.Series([False] * len(texts))
        else:
            return texts.str.contains(query, case=False, na=False)
    
    def analyze_sentiment_simple(self, texts: List[str]) -> Dict[str, int]:
        """ç°¡æ˜“ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
        positive_words = ['è‰¯ã„', 'ã‚ˆã„', 'ç´ æ™´ã‚‰ã—ã„', 'æº€è¶³', 'å¬‰ã—ã„', 'æ¥½ã—ã„', 'ä¾¿åˆ©', 'å¿«é©']
        negative_words = ['æ‚ªã„', 'ã‚ã‚‹ã„', 'ä¸æº€', 'å›°ã‚‹', 'å«Œ', 'ãƒ€ãƒ¡', 'å•é¡Œ', 'ä¸ä¾¿']
        
        sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
        
        for text in texts:
            if not text:
                sentiment_counts['neutral'] += 1
                continue
            
            pos_count = sum(1 for word in positive_words if word in text)
            neg_count = sum(1 for word in negative_words if word in text)
            
            if pos_count > neg_count:
                sentiment_counts['positive'] += 1
            elif neg_count > pos_count:
                sentiment_counts['negative'] += 1
            else:
                sentiment_counts['neutral'] += 1
        
        return sentiment_counts
    
    def generate_mock_embeddings(self, texts: List[str], dimension: int = 768) -> np.ndarray:
        """ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆå®Ÿéš›ã®Cortexé–¢æ•°ã®ä»£æ›¿ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ EMBED_TEXT_768('snowflake-arctic-embed-m-v1.5', text) ã‚’ä½¿ç”¨
        np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        embeddings = []
        
        for text in texts:
            if not text:
                # ç©ºãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
                embedding = np.zeros(dimension)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ã«åŸºã¥ã„ãŸãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ Snowflake Cortex ã® EMBED_TEXT_768 ã‚’ä½¿ç”¨
                text_hash = hash(text) % 10000
                np.random.seed(text_hash)
                
                # å˜èªã®ç¨®é¡ã«åŸºã¥ãåŸºæœ¬ãƒ™ã‚¯ãƒˆãƒ«
                base_vector = np.random.normal(0, 0.1, dimension)
                
                # æ„Ÿæƒ…çš„ãªå˜èªã«åŸºã¥ãèª¿æ•´
                positive_words = ['è‰¯ã„', 'ã‚ˆã„', 'ç´ æ™´ã‚‰ã—ã„', 'æº€è¶³', 'å¬‰ã—ã„', 'æ¥½ã—ã„', 'ä¾¿åˆ©', 'å¿«é©']
                negative_words = ['æ‚ªã„', 'ã‚ã‚‹ã„', 'ä¸æº€', 'å›°ã‚‹', 'å«Œ', 'ãƒ€ãƒ¡', 'å•é¡Œ', 'ä¸ä¾¿']
                
                pos_count = sum(1 for word in positive_words if word in text)
                neg_count = sum(1 for word in negative_words if word in text)
                
                if pos_count > neg_count:
                    base_vector[:100] += 0.3  # ãƒã‚¸ãƒ†ã‚£ãƒ–æ–¹å‘
                elif neg_count > pos_count:
                    base_vector[100:200] += 0.3  # ãƒã‚¬ãƒ†ã‚£ãƒ–æ–¹å‘
                
                # é•·ã•ã«åŸºã¥ãèª¿æ•´
                if len(text) > 50:
                    base_vector[200:300] += 0.2  # è©³ç´°ãªå›ç­”
                
                embedding = base_vector
            
            embeddings.append(embedding)
        
        return np.array(embeddings)
    
    def perform_clustering(self, embeddings: np.ndarray, n_clusters: int = None) -> Tuple[np.ndarray, int]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        if n_clusters is None:
            # ã‚¨ãƒ«ãƒœãƒ¼æ³•ã§æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¨å®š
            n_clusters = self.estimate_optimal_clusters(embeddings)
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        return cluster_labels, n_clusters
    
    def estimate_optimal_clusters(self, embeddings: np.ndarray, max_k: int = 10) -> int:
        """æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ¨å®š"""
        n_samples = len(embeddings)
        max_k = min(max_k, n_samples // 2, 8)  # å®Ÿç”¨çš„ãªç¯„å›²ã«åˆ¶é™
        
        if max_k < 2:
            return 2
        
        inertias = []
        k_range = range(2, max_k + 1)
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(embeddings)
            inertias.append(kmeans.inertia_)
        
        # ã‚¨ãƒ«ãƒœãƒ¼æ³•ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ¨å®š
        if len(inertias) >= 2:
            differences = np.diff(inertias)
            if len(differences) >= 2:
                second_differences = np.diff(differences)
                optimal_k = np.argmax(second_differences) + 2
                return min(optimal_k, max_k)
        
        return min(3, max_k)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def find_similar_responses(self, query_text: str, embeddings: np.ndarray, 
                             texts: List[str], top_k: int = 5) -> List[Tuple[str, float]]:
        """æ„å‘³çš„é¡ä¼¼æ¤œç´¢"""
        # ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = self.generate_mock_embeddings([query_text])[0]
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = cosine_similarity([query_embedding], embeddings)[0]
        
        # ä¸Šä½kä»¶å–å¾—
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # æœ€å°é¡ä¼¼åº¦é–¾å€¤
                results.append((texts[idx], similarities[idx]))
        
        return results
    
    def analyze_clusters(self, embeddings: np.ndarray, cluster_labels: np.ndarray, 
                        texts: List[str]) -> Dict[int, Dict]:
        """ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ"""
        cluster_analysis = {}
        
        for cluster_id in np.unique(cluster_labels):
            cluster_mask = cluster_labels == cluster_id
            cluster_texts = [text for i, text in enumerate(texts) if cluster_mask[i]]
            cluster_embeddings = embeddings[cluster_mask]
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ã®ç‰¹å¾´æŠ½å‡º
            word_freq = self.extract_keywords(cluster_texts, min_length=2)
            top_words = dict(word_freq.most_common(10))
            
            # ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ä»£è¡¨çš„ãªãƒ†ã‚­ã‚¹ãƒˆ
            cluster_center = np.mean(cluster_embeddings, axis=0)
            distances = np.linalg.norm(cluster_embeddings - cluster_center, axis=1)
            representative_idx = np.argmin(distances)
            representative_text = cluster_texts[representative_idx]
            
            # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
            sentiment = self.analyze_sentiment_simple(cluster_texts)
            
            cluster_analysis[cluster_id] = {
                'size': len(cluster_texts),
                'top_words': top_words,
                'representative_text': representative_text,
                'sentiment': sentiment,
                'texts': cluster_texts[:5]  # æœ€åˆã®5ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆä¾‹
            }
        
        return cluster_analysis
    
    def reduce_dimensions(self, embeddings: np.ndarray, method: str = 'tsne') -> np.ndarray:
        """æ¬¡å…ƒå‰Šæ¸›"""
        if method == 'tsne':
            if len(embeddings) > 3:
                reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))
                return reducer.fit_transform(embeddings)
            else:
                return np.random.rand(len(embeddings), 2)
        elif method == 'pca':
            reducer = PCA(n_components=2, random_state=42)
            return reducer.fit_transform(embeddings)
        elif method == 'umap':
            # UMAPã®ä»£ã‚ã‚Šã«ç°¡æ˜“å®Ÿè£…
            if len(embeddings) > 10:
                reducer = PCA(n_components=2, random_state=42)
                return reducer.fit_transform(embeddings)
            else:
                return np.random.rand(len(embeddings), 2)
        
        return embeddings[:, :2]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def generate_wordcloud_data(self, texts: List[str]) -> Dict[str, int]:
        """ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            combined_text = ' '.join(texts)
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keywords = self.extract_keywords(texts, min_length=2)
            
            # ä¸Šä½50å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿”ã™
            return dict(keywords.most_common(50))
            
        except Exception as e:
            st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}
    
    def create_network_graph(self, embeddings: np.ndarray, texts: List[str], 
                           similarity_threshold: float = 0.7) -> dict:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ä½œæˆ"""
        try:
            # é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—
            similarity_matrix = cosine_similarity(embeddings)
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ä½œæˆ
            G = nx.Graph()
            
            # ãƒãƒ¼ãƒ‰è¿½åŠ 
            for i, text in enumerate(texts):
                G.add_node(i, text=text[:50] + "..." if len(text) > 50 else text)
            
            # ã‚¨ãƒƒã‚¸è¿½åŠ ï¼ˆé¡ä¼¼åº¦ãŒé–¾å€¤ä»¥ä¸Šã®å ´åˆï¼‰
            for i in range(len(texts)):
                for j in range(i+1, len(texts)):
                    if similarity_matrix[i][j] > similarity_threshold:
                        G.add_edge(i, j, weight=similarity_matrix[i][j])
            
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—
            pos = nx.spring_layout(G, k=1, iterations=50)
            
            # Plotlyç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            edge_x = []
            edge_y = []
            for edge in G.edges():
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                edge_x.extend([x0, x1, None])
                edge_y.extend([y0, y1, None])
            
            node_x = [pos[node][0] for node in G.nodes()]
            node_y = [pos[node][1] for node in G.nodes()]
            node_text = [G.nodes[node]['text'] for node in G.nodes()]
            
            return {
                'edge_x': edge_x,
                'edge_y': edge_y,
                'node_x': node_x,
                'node_y': node_y,
                'node_text': node_text,
                'num_nodes': len(G.nodes()),
                'num_edges': len(G.edges())
            }
            
        except Exception as e:
            st.error(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def analyze_temporal_trends(self, df: pd.DataFrame) -> Dict:
        """æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        try:
            # æ—¥åˆ¥ã®å›ç­”æ•°ã¨ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
            daily_stats = df.groupby(df['response_date'].dt.date).agg({
                'text_response': 'count',
                'respondent_id': 'nunique'
            }).rename(columns={
                'text_response': 'response_count',
                'respondent_id': 'unique_respondents'
            })
            
            # é€±åˆ¥ã®æ–‡å­—æ•°ãƒˆãƒ¬ãƒ³ãƒ‰
            df['week'] = df['response_date'].dt.to_period('W')
            weekly_char_length = df.groupby('week')['text_response'].apply(
                lambda x: x.str.len().mean()
            )
            
            # æœˆåˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ³ãƒ‰
            df['month'] = df['response_date'].dt.to_period('M')
            monthly_keywords = {}
            
            for month in df['month'].unique():
                month_texts = df[df['month'] == month]['text_response'].tolist()
                keywords = self.extract_keywords(month_texts, min_length=2)
                monthly_keywords[str(month)] = dict(keywords.most_common(10))
            
            return {
                'daily_stats': daily_stats,
                'weekly_char_length': weekly_char_length,
                'monthly_keywords': monthly_keywords
            }
            
        except Exception as e:
            st.error(f"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def generate_insights_report(self, df: pd.DataFrame) -> Dict[str, str]:
        """è©³ç´°ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        insights = {}
        
        # åŸºæœ¬çµ±è¨ˆã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        total_responses = len(df)
        avg_length = df['text_response'].str.len().mean()
        unique_respondents = df['respondent_id'].nunique()
        date_range = (df['response_date'].max() - df['response_date'].min()).days
        
        insights['basic_stats'] = f"""
        **åŸºæœ¬çµ±è¨ˆã‚µãƒãƒªãƒ¼:**
        - ç·å›ç­”æ•°: {total_responses:,}ä»¶
        - ãƒ¦ãƒ‹ãƒ¼ã‚¯å›ç­”è€…æ•°: {unique_respondents:,}äºº
        - å¹³å‡æ–‡å­—æ•°: {avg_length:.1f}æ–‡å­—
        - ãƒ‡ãƒ¼ã‚¿åé›†æœŸé–“: {date_range}æ—¥é–“
        - 1æ—¥ã‚ãŸã‚Šå¹³å‡å›ç­”æ•°: {total_responses/max(date_range, 1):.1f}ä»¶
        """
        
        # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        sentiment_results = self.analyze_sentiment_simple(df['text_response'].tolist())
        total = sum(sentiment_results.values())
        pos_ratio = sentiment_results['positive'] / total
        neg_ratio = sentiment_results['negative'] / total
        
        sentiment_interpretation = ""
        if pos_ratio > 0.6:
            sentiment_interpretation = "éå¸¸ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå‚¾å‘"
        elif pos_ratio > 0.4:
            sentiment_interpretation = "ã‚„ã‚„æº€è¶³ã—ã¦ã„ã‚‹å‚¾å‘"
        elif neg_ratio > 0.4:
            sentiment_interpretation = "æ”¹å–„ãŒå¿…è¦ãªçŠ¶æ³"
        else:
            sentiment_interpretation = "ä¸­æ€§çš„ãªè©•ä¾¡"
        
        insights['sentiment'] = f"""
        **ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ:**
        - ãƒã‚¸ãƒ†ã‚£ãƒ–: {sentiment_results['positive']:,}ä»¶ ({pos_ratio:.1%})
        - ãƒã‚¬ãƒ†ã‚£ãƒ–: {sentiment_results['negative']:,}ä»¶ ({neg_ratio:.1%})
        - ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«: {sentiment_results['neutral']:,}ä»¶ ({(1-pos_ratio-neg_ratio):.1%})
        - ç·åˆè©•ä¾¡: {sentiment_interpretation}
        """
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if self.clusters is not None:
            cluster_sizes = [info['size'] for info in self.clusters.values()]
            largest_cluster_id = max(self.clusters.keys(), key=lambda k: self.clusters[k]['size'])
            largest_cluster = self.clusters[largest_cluster_id]
            
            insights['clusters'] = f"""
            **ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ:**
            - æ¤œå‡ºã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(self.clusters)}å€‹
            - æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º: {max(cluster_sizes)}ä»¶
            - å¹³å‡ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º: {np.mean(cluster_sizes):.1f}ä»¶
            - ä¸»è¦ãƒ†ãƒ¼ãƒ: {largest_cluster['representative_text'][:100]}...
            - ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(list(largest_cluster['top_words'].keys())[:5])}
            """
        
        # æ™‚ç³»åˆ—ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if date_range > 1:
            daily_counts = df.groupby(df['response_date'].dt.date).size()
            peak_day = daily_counts.idxmax()
            peak_count = daily_counts.max()
            
            insights['temporal'] = f"""
            **æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³:**
            - æœ€ã‚‚å›ç­”ãŒå¤šã„æ—¥: {peak_day} ({peak_count}ä»¶)
            - é€±æœ« vs å¹³æ—¥ã®å›ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãŒå¯èƒ½
            - ç¶™ç¶šçš„ãªå›ç­”åé›†: {date_range}æ—¥é–“
            """
        
        return insights
    
    def create_comprehensive_dashboard(self, df: pd.DataFrame) -> Dict:
        """åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        dashboard_data = {}
        
        # KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹
        dashboard_data['kpis'] = {
            'total_responses': len(df),
            'unique_respondents': df['respondent_id'].nunique(),
            'avg_response_length': df['text_response'].str.len().mean(),
            'response_rate': len(df) / df['respondent_id'].nunique() if df['respondent_id'].nunique() > 0 else 0,
            'data_quality_score': (1 - df['text_response'].isna().sum() / len(df)) * 100
        }
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿
        if hasattr(self, 'trend_analysis') and self.trend_analysis:
            dashboard_data['trends'] = self.trend_analysis
        
        # ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        all_keywords = self.extract_keywords(df['text_response'].tolist())
        dashboard_data['top_keywords'] = dict(all_keywords.most_common(15))
        
        return dashboard_data

# ãƒ¡ã‚¤ãƒ³åˆ†æã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
@st.cache_data
def initialize_analyzer():
    return SurveyAnalyzer()

analyzer = initialize_analyzer()

# ãƒ¡ã‚¤ãƒ³UI
st.markdown('<h1 class="main-header">ğŸ“Š ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«</h1>', unsafe_allow_html=True)

st.markdown("""
ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€CSVã‚„Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã€é »åº¦åˆ†æã€åŸºæœ¬çš„ãªå¯è¦–åŒ–æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

**æ‰‹é †:**
1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
2. å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’é¸æŠ
3. åˆ†æé–‹å§‹
""")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'data_uploaded' not in st.session_state:
    st.session_state.data_uploaded = False
if 'columns_configured' not in st.session_state:
    st.session_state.columns_configured = False

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("ğŸ”§ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.subheader("ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader(
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['csv', 'xlsx', 'xls'],
        help="CSVï¼ˆUTF-8æ¨å¥¨ï¼‰ã¾ãŸã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    if uploaded_file is not None:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        try:
            if uploaded_file.name.endswith('.csv'):
                # æ–‡å­—ã‚³ãƒ¼ãƒ‰è‡ªå‹•åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
                raw_data = uploaded_file.read()
                try:
                    df_raw = pd.read_csv(io.StringIO(raw_data.decode('utf-8')))
                except UnicodeDecodeError:
                    df_raw = pd.read_csv(io.StringIO(raw_data.decode('shift_jis')))
            else:
                df_raw = pd.read_excel(uploaded_file)
            
            analyzer.raw_data = df_raw
            st.session_state.data_uploaded = True
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({len(df_raw)}è¡Œ, {len(df_raw.columns)}åˆ—)")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            with st.expander("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                st.dataframe(df_raw.head(), use_container_width=True)
            
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.session_state.data_uploaded = False
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚«ãƒ©ãƒ é¸æŠ
    if st.session_state.data_uploaded and analyzer.raw_data is not None:
        st.subheader("ğŸ·ï¸ ã‚¹ãƒ†ãƒƒãƒ—2: ã‚«ãƒ©ãƒ é¸æŠ")
        
        columns = analyzer.raw_data.columns.tolist()
        
        # å›ç­”è€…IDã‚«ãƒ©ãƒ é¸æŠ
        respondent_col = st.selectbox(
            "å›ç­”è€…ID ã‚«ãƒ©ãƒ ",
            options=columns,
            help="å„å›ç­”è€…ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # å›ç­”æ—¥æ™‚ã‚«ãƒ©ãƒ é¸æŠ
        date_col = st.selectbox(
            "å›ç­”æ—¥æ™‚ ã‚«ãƒ©ãƒ ", 
            options=columns,
            help="å›ç­”ã—ãŸæ—¥æ™‚ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆå›ç­”ã‚«ãƒ©ãƒ é¸æŠ
        text_col = st.selectbox(
            "ãƒ†ã‚­ã‚¹ãƒˆå›ç­” ã‚«ãƒ©ãƒ ",
            options=columns,
            help="åˆ†æå¯¾è±¡ã¨ãªã‚‹ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆå›ç­”ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # è¨­å®šç¢ºèªãƒœã‚¿ãƒ³
        if st.button("âœ… ã‚«ãƒ©ãƒ è¨­å®šã‚’ç¢ºèª", type="primary"):
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            is_valid, errors = analyzer.validate_data(analyzer.raw_data, respondent_col, date_col, text_col)
            
            if not is_valid:
                st.error("âš ï¸ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼:")
                for error in errors:
                    st.error(f"â€¢ {error}")
            else:
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                analyzer.data = analyzer.prepare_data(analyzer.raw_data, respondent_col, date_col, text_col) 
                analyzer.processed_data = analyzer.data.copy()
                analyzer.processed_data['text_response'] = analyzer.processed_data['text_response'].apply(analyzer.preprocess_text)
                
                analyzer.column_mapping = {
                    'respondent_id': respondent_col,
                    'response_date': date_col,
                    'text_response': text_col
                }
                
                st.session_state.columns_configured = True
                st.success(f"âœ… åˆ†ææº–å‚™å®Œäº† ({len(analyzer.data)}ä»¶ã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿)")
                st.rerun()
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†æè¨­å®š
    if st.session_state.columns_configured and analyzer.processed_data is not None:
        st.subheader("âš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†æè¨­å®š")
        
        df = analyzer.processed_data
        
        # ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
        st.markdown("**ğŸ“… æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿**")
        date_range = st.date_input(
            "åˆ†ææœŸé–“ã‚’é¸æŠ",
            value=(df['response_date'].min().date(), df['response_date'].max().date()),
            min_value=df['response_date'].min().date(),
            max_value=df['response_date'].max().date()
        )
        
        if len(date_range) == 2:
            start_date, end_date = date_range
            mask = (df['response_date'].dt.date >= start_date) & (df['response_date'].dt.date <= end_date)
            analyzer.processed_data = analyzer.processed_data[mask]
        
        st.markdown("**ğŸ” åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
        min_word_length = st.slider("æœ€å°å˜èªé•·", 1, 5, 2)
        top_n_words = st.slider("è¡¨ç¤ºã™ã‚‹ä¸Šä½å˜èªæ•°", 10, 50, 20)
        
        # è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ã®ã‚«ãƒ©ãƒ ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        other_columns = [col for col in analyzer.raw_data.columns 
                        if col not in [analyzer.column_mapping['respondent_id'], 
                                     analyzer.column_mapping['response_date'], 
                                     analyzer.column_mapping['text_response']]]
        
        if other_columns:
            st.markdown("**ğŸ¯ è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿**")
            selected_filter_col = st.selectbox(
                "ãƒ•ã‚£ãƒ«ã‚¿ç”¨ã‚«ãƒ©ãƒ ï¼ˆä»»æ„ï¼‰",
                options=['ãªã—'] + other_columns
            )
            
            if selected_filter_col != 'ãªã—':
                unique_values = analyzer.raw_data[selected_filter_col].unique()
                selected_values = st.multiselect(
                    f"{selected_filter_col} ã®å€¤ã‚’é¸æŠ",
                    options=unique_values,
                    default=unique_values
                )
                
                if selected_values:
                    # å…ƒãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    original_mask = analyzer.raw_data[selected_filter_col].isin(selected_values)
                    # prepared_dataã«å¯¾å¿œã™ã‚‹ãƒã‚¹ã‚¯ã‚’ä½œæˆ
                    filtered_raw = analyzer.raw_data[original_mask]
                    filtered_prepared = analyzer.prepare_data(
                        filtered_raw, 
                        analyzer.column_mapping['respondent_id'],
                        analyzer.column_mapping['response_date'], 
                        analyzer.column_mapping['text_response']
                    )
                    analyzer.processed_data = filtered_prepared
                    analyzer.processed_data['text_response'] = analyzer.processed_data['text_response'].apply(analyzer.preprocess_text)

        # Phase 2: AIåˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.markdown("**ğŸ¤– AIåˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
        enable_ai_analysis = st.checkbox("AIåˆ†æã‚’æœ‰åŠ¹åŒ–", value=True, help="ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½")
        
        if enable_ai_analysis:
            similarity_threshold = st.slider("é¡ä¼¼åº¦é–¾å€¤", 0.1, 0.9, 0.3, 0.1)
            n_clusters = st.selectbox("ã‚¯ãƒ©ã‚¹ã‚¿æ•°", ['è‡ªå‹•', 3, 4, 5, 6, 7, 8], help="è‡ªå‹•ï¼šæœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¨å®š")
            dimension_reduction_method = st.selectbox("æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•", ['t-SNE', 'PCA', 'UMAP'])
            
            # Phase 3: é«˜åº¦ãªå¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½
            st.markdown("**ğŸ“Š é«˜åº¦ãªå¯è¦–åŒ–**")
            enable_advanced_viz = st.checkbox("é«˜åº¦ãªå¯è¦–åŒ–ã‚’æœ‰åŠ¹åŒ–", value=False, help="ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ç­‰")
            
            if enable_advanced_viz:
                wordcloud_options = st.multiselect(
                    "ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ", 
                    ["å…¨ä½“", "ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥"], 
                    default=["å…¨ä½“"]
                )
                network_analysis = st.checkbox("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ", help="å›ç­”é–“ã®é–¢é€£æ€§ã‚’å¯è¦–åŒ–")
                trend_analysis = st.checkbox("ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ", help="æ™‚ç³»åˆ—ã§ã®å¤‰åŒ–ã‚’åˆ†æ")
            
            # Phase 3: ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½
            st.markdown("**ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½**")
            enable_reports = st.checkbox("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’æœ‰åŠ¹åŒ–", value=False)
            
            if enable_reports:
                report_type = st.selectbox(
                    "ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼",
                    ["è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ", "ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼", "ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ãƒãƒ¼ãƒˆ"]
                )
                include_visualizations = st.checkbox("å¯è¦–åŒ–ã‚’å«ã‚ã‚‹", value=True)
                
                if st.button("ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", type="secondary"):
                    with st.spinner("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­..."):
                        insights = analyzer.generate_insights_report(analyzer.processed_data)
                        st.session_state.report_insights = insights
                        
                        if analyzer.trend_analysis is None:
                            analyzer.trend_analysis = analyzer.analyze_temporal_trends(analyzer.processed_data)
                    
                    st.success("âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
                    st.rerun()
            
            # AIåˆ†æå®Ÿè¡Œ
            if st.button("ğŸš€ AIåˆ†æå®Ÿè¡Œ", type="primary"):
                with st.spinner("ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­..."):
                    texts = analyzer.processed_data['text_response'].tolist()
                    analyzer.embeddings = analyzer.generate_mock_embeddings(texts)
                    
                with st.spinner("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­..."):
                    n_clusters_val = None if n_clusters == 'è‡ªå‹•' else int(n_clusters)
                    analyzer.cluster_labels, actual_clusters = analyzer.perform_clustering(
                        analyzer.embeddings, n_clusters_val
                    )
                    analyzer.clusters = analyzer.analyze_clusters(
                        analyzer.embeddings, analyzer.cluster_labels, texts
                    )
                
                # Phase 3: é«˜åº¦ãªåˆ†æã‚‚å®Ÿè¡Œ
                if enable_advanced_viz:
                    with st.spinner("é«˜åº¦ãªåˆ†æå®Ÿè¡Œä¸­..."):
                        if trend_analysis:
                            analyzer.trend_analysis = analyzer.analyze_temporal_trends(analyzer.processed_data)
                        
                        if network_analysis:
                            analyzer.network_graph = analyzer.create_network_graph(
                                analyzer.embeddings, texts, similarity_threshold=0.6
                            )
                
                st.success(f"âœ… AIåˆ†æå®Œäº† ({actual_clusters}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ¤œå‡º)")
                st.rerun()

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
if st.session_state.columns_configured and analyzer.processed_data is not None:
    df = analyzer.processed_data
    
    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç·å›ç­”æ•°", len(df))
    
    with col2:
        avg_length = df['text_response'].str.len().mean()
        st.metric("å¹³å‡æ–‡å­—æ•°", f"{avg_length:.1f}")
    
    with col3:
        unique_respondents = df['respondent_id'].nunique()
        st.metric("å›ç­”è€…æ•°", unique_respondents)
    
    with col4:
        date_range_days = (df['response_date'].max() - df['response_date'].min()).days
        st.metric("åˆ†ææœŸé–“", f"{date_range_days}æ—¥é–“")
    
    # ã‚¿ãƒ–ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†å‰²
    if analyzer.embeddings is not None:
        tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
            "ğŸ” æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿", "ğŸ“Š é »åº¦åˆ†æ", "ğŸ“ˆ å¯è¦–åŒ–", 
            "ğŸ’­ ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ", "ğŸ¯ æ„å‘³çš„æ¤œç´¢", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ",
            "ğŸ¨ é«˜åº¦ãªå¯è¦–åŒ–", "ğŸ“‹ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ"
        ])
    else:
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿", "ğŸ“Š é »åº¦åˆ†æ", "ğŸ“ˆ å¯è¦–åŒ–", "ğŸ’­ ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"])
    
    with tab1:
        st.markdown('<h2 class="sub-header">ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢</h2>', unsafe_allow_html=True)
        
        col1, col2 = st.columns([3, 1])
        with col1:
            search_query = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ä¾‹: æº€è¶³, å•é¡Œ")
        with col2:
            use_regex = st.checkbox("æ­£è¦è¡¨ç¾", help="æ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªæ¤œç´¢")
        
        if search_query:
            search_results = analyzer.search_responses(df['text_response'], search_query, use_regex)
            filtered_df = df[search_results]
            
            st.info(f"ğŸ” {len(filtered_df)}ä»¶ã®å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            if len(filtered_df) > 0:
                # æ¤œç´¢çµæœè¡¨ç¤º
                st.subheader("æ¤œç´¢çµæœ")
                display_df = filtered_df[['respondent_id', 'response_date', 'text_response']].copy()
                display_df['response_date'] = display_df['response_date'].dt.strftime('%Y-%m-%d')
                
                st.dataframe(
                    display_df,
                    use_container_width=True,
                    height=400
                )
                
                # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                csv = display_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"search_results_{search_query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
    
    with tab2:
        st.markdown('<h2 class="sub-header">é »åº¦åˆ†æ</h2>', unsafe_allow_html=True)
        
        # å˜èªé »åº¦åˆ†æ
        st.subheader("å˜èªé »åº¦")
        word_freq = analyzer.extract_keywords(df['text_response'].tolist(), min_word_length)
        top_words = dict(word_freq.most_common(top_n_words))
        
        if top_words:
            words_df = pd.DataFrame(list(top_words.items()), columns=['å˜èª', 'å‡ºç¾å›æ•°'])
            st.dataframe(words_df, use_container_width=True)
        
        # N-gramåˆ†æ
        st.subheader("2-gramåˆ†æ")
        bigrams = analyzer.extract_ngrams(df['text_response'].tolist(), 2)
        top_bigrams = dict(bigrams.most_common(20))
        
        if top_bigrams:
            bigrams_df = pd.DataFrame(list(top_bigrams.items()), columns=['2-gram', 'å‡ºç¾å›æ•°'])
            st.dataframe(bigrams_df, use_container_width=True)
    
    with tab3:
        st.markdown('<h2 class="sub-header">å¯è¦–åŒ–</h2>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # é »å‡ºå˜èªã®æ£’ã‚°ãƒ©ãƒ•
            if top_words:
                fig_words = px.bar(
                    x=list(top_words.values()),
                    y=list(top_words.keys()),
                    orientation='h',
                    title=f"é »å‡ºå˜èª TOP{len(top_words)}",
                    labels={'x': 'å‡ºç¾å›æ•°', 'y': 'å˜èª'}
                )
                fig_words.update_layout(height=600, yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig_words, use_container_width=True)
        
        with col2:
            # å›ç­”æ•°ã®æ™‚ç³»åˆ—æ¨ç§»
            daily_counts = df.groupby(df['response_date'].dt.date).size().reset_index()
            daily_counts.columns = ['æ—¥ä»˜', 'å›ç­”æ•°']
            
            fig_timeline = px.line(
                daily_counts,
                x='æ—¥ä»˜',
                y='å›ç­”æ•°',
                title='å›ç­”æ•°ã®æ¨ç§»',
                markers=True
            )
            fig_timeline.update_layout(height=400)
            st.plotly_chart(fig_timeline, use_container_width=True)
        
        # æ–‡å­—æ•°åˆ†å¸ƒ
        st.subheader("æ–‡å­—æ•°åˆ†å¸ƒ")
        char_lengths = df['text_response'].str.len()
        
        fig_hist = px.histogram(
            x=char_lengths,
            nbins=30,
            title='å›ç­”ã®æ–‡å­—æ•°åˆ†å¸ƒ',
            labels={'x': 'æ–‡å­—æ•°', 'y': 'å›ç­”æ•°'}
        )
        fig_hist.update_layout(height=400)
        st.plotly_chart(fig_hist, use_container_width=True)
        
        # çµ±è¨ˆæƒ…å ±
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æœ€å°æ–‡å­—æ•°", char_lengths.min())
        with col2:
            st.metric("æœ€å¤§æ–‡å­—æ•°", char_lengths.max())
        with col3:
            st.metric("ä¸­å¤®å€¤", char_lengths.median())
        with col4:
            st.metric("æ¨™æº–åå·®", f"{char_lengths.std():.1f}")
    
    with tab4:
        st.markdown('<h2 class="sub-header">ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ</h2>', unsafe_allow_html=True)
        
        # ç°¡æ˜“ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
        sentiment_results = analyzer.analyze_sentiment_simple(df['text_response'].tolist())
        
        col1, col2 = st.columns(2)
        
        with col1:
            # å††ã‚°ãƒ©ãƒ•
            fig_pie = px.pie(
                values=list(sentiment_results.values()),
                names=['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'],
                title='ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†å¸ƒ',
                color_discrete_map={
                    'ãƒã‚¸ãƒ†ã‚£ãƒ–': '#10B981',
                    'ãƒã‚¬ãƒ†ã‚£ãƒ–': '#EF4444',
                    'ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«': '#6B7280'
                }
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col2:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
            total_responses = sum(sentiment_results.values())
            
            st.metric(
                "ãƒã‚¸ãƒ†ã‚£ãƒ–ç‡",
                f"{sentiment_results['positive']/total_responses*100:.1f}%",
                delta=f"{sentiment_results['positive']}ä»¶"
            )
            st.metric(
                "ãƒã‚¬ãƒ†ã‚£ãƒ–ç‡", 
                f"{sentiment_results['negative']/total_responses*100:.1f}%",
                delta=f"{sentiment_results['negative']}ä»¶"
            )
            st.metric(
                "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ç‡",
                f"{sentiment_results['neutral']/total_responses*100:.1f}%",
                delta=f"{sentiment_results['neutral']}ä»¶"
            )
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
        st.subheader("ğŸ“ åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
        
        insights = []
        
        # å›ç­”ç‡ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if len(df) > 0:
            insights.append(f"â€¢ ç·å›ç­”æ•°ã¯{len(df)}ä»¶ã§ã€å¹³å‡æ–‡å­—æ•°ã¯{avg_length:.1f}æ–‡å­—ã§ã™")
        
        # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        pos_ratio = sentiment_results['positive'] / total_responses
        neg_ratio = sentiment_results['negative'] / total_responses
        
        if pos_ratio > 0.5:
            insights.append("â€¢ å…¨ä½“çš„ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå›ç­”ãŒå¤šãã€æº€è¶³åº¦ãŒé«˜ã„å‚¾å‘ã«ã‚ã‚Šã¾ã™")
        elif neg_ratio > 0.3:
            insights.append("â€¢ ãƒã‚¬ãƒ†ã‚£ãƒ–ãªå›ç­”ãŒæ¯”è¼ƒçš„å¤šãã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
        else:
            insights.append("â€¢ ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã¾ã™")
        
        # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if top_words:
            most_common_word = list(top_words.keys())[0]
            insights.append(f"â€¢ æœ€ã‚‚é »å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã€Œ{most_common_word}ã€({top_words[most_common_word]}å›å‡ºç¾)")
        
        # æ–‡å­—æ•°ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if char_lengths.std() > char_lengths.mean():
            insights.append("â€¢ å›ç­”ã®æ–‡å­—æ•°ã«ã°ã‚‰ã¤ããŒå¤§ããã€è©³ç´°ãªå›ç­”ã¨ç°¡æ½”ãªå›ç­”ãŒæ··åœ¨ã—ã¦ã„ã¾ã™")
        
        for insight in insights:
            st.markdown(f'<div class="insight-box">{insight}</div>', unsafe_allow_html=True)
    
    # Phase 2: AIåˆ†æã‚¿ãƒ–
    if analyzer.embeddings is not None:
        with tab5:
            st.markdown('<h2 class="sub-header">æ„å‘³çš„é¡ä¼¼æ¤œç´¢</h2>', unsafe_allow_html=True)
            
            st.markdown("""
            ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã§ã¯ãªãã€**æ„å‘³çš„ãªé¡ä¼¼æ€§**ã«åŸºã¥ã„ã¦å›ç­”ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
            ä¾‹ï¼šã€Œæº€è¶³ã€ã§æ¤œç´¢ã™ã‚‹ã¨ã€Œå¬‰ã—ã„ã€ã€Œè‰¯ã„ã€ãªã©ã‚‚å«ã‚€å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã™ã€‚
            """)
            
            col1, col2 = st.columns([3, 1])
            with col1:
                semantic_query = st.text_input(
                    "æ„å‘³çš„æ¤œç´¢ã‚¯ã‚¨ãƒª", 
                    placeholder="ä¾‹: æº€è¶³ã—ã¦ã„ã‚‹, å•é¡ŒãŒã‚ã‚‹, æ”¹å–„ã—ã¦ã»ã—ã„"
                )
            with col2:
                top_k_similar = st.number_input("è¡¨ç¤ºä»¶æ•°", 1, 20, 5)
            
            if semantic_query:
                similar_responses = analyzer.find_similar_responses(
                    semantic_query,
                    analyzer.embeddings,
                    df['text_response'].tolist(),
                    top_k_similar
                )
                
                if similar_responses:
                    st.subheader(f"ğŸ¯ é¡ä¼¼å›ç­” (ä¸Šä½{len(similar_responses)}ä»¶)")
                    
                    for i, (text, similarity) in enumerate(similar_responses, 1):
                        with st.expander(f"{i}. é¡ä¼¼åº¦: {similarity:.3f}"):
                            st.write(text)
                            
                            # è©²å½“ã™ã‚‹å›ç­”è€…æƒ…å ±ã‚‚è¡¨ç¤º
                            matching_row = df[df['text_response'] == text].iloc[0]
                            st.caption(f"å›ç­”è€…ID: {matching_row['respondent_id']} | å›ç­”æ—¥: {matching_row['response_date'].strftime('%Y-%m-%d')}")
                else:
                    st.info("é¡ä¼¼ã™ã‚‹å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
        
        with tab6:
            st.markdown('<h2 class="sub-header">ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ</h2>', unsafe_allow_html=True)
            
            if analyzer.clusters is not None:
                st.markdown("**å›ç­”ã‚’AIãŒè‡ªå‹•çš„ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã—ã¾ã—ãŸã€‚é¡ä¼¼ã—ãŸãƒ†ãƒ¼ãƒã®å›ç­”ãŒåŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚**")
                
                # ã‚¯ãƒ©ã‚¹ã‚¿æ¦‚è¦
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ã‚¯ãƒ©ã‚¹ã‚¿æ•°", len(analyzer.clusters))
                with col2:
                    largest_cluster = max(analyzer.clusters.values(), key=lambda x: x['size'])
                    st.metric("æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º", largest_cluster['size'])
                with col3:
                    avg_cluster_size = np.mean([cluster['size'] for cluster in analyzer.clusters.values()])
                    st.metric("å¹³å‡ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º", f"{avg_cluster_size:.1f}")
                
                # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®è©³ç´°
                st.subheader("ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿è©³ç´°åˆ†æ")
                
                for cluster_id, cluster_info in analyzer.clusters.items():
                    with st.expander(f"ğŸ·ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ({cluster_info['size']}ä»¶)", expanded=cluster_id==0):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**ä»£è¡¨çš„ãªå›ç­”:**")
                            st.write(f"_{cluster_info['representative_text']}_")
                            
                            st.markdown("**é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:**")
                            top_words = cluster_info['top_words']
                            if top_words:
                                words_text = ", ".join([f"{word}({count})" for word, count in list(top_words.items())[:8]])
                                st.write(words_text)
                        
                        with col2:
                            st.markdown("**ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†å¸ƒ:**")
                            sentiment = cluster_info['sentiment']
                            
                            fig_cluster_sentiment = px.pie(
                                values=list(sentiment.values()),
                                names=['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'],
                                title=f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ",
                                color_discrete_map={
                                    'ãƒã‚¸ãƒ†ã‚£ãƒ–': '#10B981',
                                    'ãƒã‚¬ãƒ†ã‚£ãƒ–': '#EF4444', 
                                    'ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«': '#6B7280'
                                }
                            )
                            fig_cluster_sentiment.update_layout(height=300, showlegend=False)
                            st.plotly_chart(fig_cluster_sentiment, use_container_width=True)
                        
                        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
                        st.markdown("**ã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ã®å›ç­”ä¾‹:**")
                        for j, sample_text in enumerate(cluster_info['texts'][:3], 1):
                            st.write(f"{j}. {sample_text}")
                
                # 2Då¯è¦–åŒ–
                st.subheader("ğŸ—ºï¸ ã‚¯ãƒ©ã‚¹ã‚¿ã®2Då¯è¦–åŒ–")
                
                # æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•é¸æŠ
                method_map = {'t-SNE': 'tsne', 'PCA': 'pca', 'UMAP': 'umap'}
                selected_method = method_map[dimension_reduction_method]
                
                with st.spinner(f"{dimension_reduction_method}ã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ä¸­..."):
                    reduced_embeddings = analyzer.reduce_dimensions(analyzer.embeddings, selected_method)
                
                # æ•£å¸ƒå›³ä½œæˆ
                plot_df = pd.DataFrame({
                    'x': reduced_embeddings[:, 0],
                    'y': reduced_embeddings[:, 1], 
                    'cluster': [f'ã‚¯ãƒ©ã‚¹ã‚¿ {label}' for label in analyzer.cluster_labels],
                    'text': df['text_response'].tolist(),
                    'respondent_id': df['respondent_id'].tolist()
                })
                
                fig_scatter = px.scatter(
                    plot_df,
                    x='x', y='y',
                    color='cluster',
                    hover_data=['respondent_id'],
                    title=f"ã‚¯ãƒ©ã‚¹ã‚¿å¯è¦–åŒ– ({dimension_reduction_method})",
                    labels={'x': f'{dimension_reduction_method} 1', 'y': f'{dimension_reduction_method} 2'}
                )
                
                fig_scatter.update_traces(
                    hovertemplate="<b>%{hovertext}</b><br>" +
                                "å›ç­”è€…ID: %{customdata[0]}<br>" +
                                "<extra></extra>",
                    hovertext=[text[:100] + "..." if len(text) > 100 else text for text in plot_df['text']]
                )
                
                fig_scatter.update_layout(height=600)
                st.plotly_chart(fig_scatter, use_container_width=True)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºåˆ†å¸ƒ
                st.subheader("ğŸ“ˆ ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºåˆ†å¸ƒ")
                cluster_sizes = [cluster_info['size'] for cluster_info in analyzer.clusters.values()]
                cluster_names = [f"ã‚¯ãƒ©ã‚¹ã‚¿ {i}" for i in range(len(cluster_sizes))]
                
                fig_cluster_sizes = px.bar(
                    x=cluster_names,
                    y=cluster_sizes,
                    title="å„ã‚¯ãƒ©ã‚¹ã‚¿ã®å›ç­”æ•°",
                    labels={'x': 'ã‚¯ãƒ©ã‚¹ã‚¿', 'y': 'å›ç­”æ•°'}
                )
                fig_cluster_sizes.update_layout(height=400)
                st.plotly_chart(fig_cluster_sizes, use_container_width=True)
                
            else:
                st.info("ğŸ‘† å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€ŒAIåˆ†æå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æã‚’é–‹å§‹ã—ã¦ãã ã•ã„")
        
        # Phase 3: é«˜åº¦ãªå¯è¦–åŒ–ã‚¿ãƒ–
        with tab7:
            st.markdown('<h2 class="sub-header">é«˜åº¦ãªå¯è¦–åŒ–</h2>', unsafe_allow_html=True)
            
            if analyzer.embeddings is not None:
                viz_option = st.selectbox(
                    "å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
                    ["ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•", "3Dæ•£å¸ƒå›³", "æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰"]
                )
                
                if viz_option == "ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰":
                    st.subheader("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                    
                    wc_type = st.radio("è¡¨ç¤ºã‚¿ã‚¤ãƒ—", ["å…¨ä½“", "ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥"])
                    
                    if wc_type == "å…¨ä½“":
                        texts = df['text_response'].tolist()
                        wordcloud_data = analyzer.generate_wordcloud_data(texts)
                        
                        if wordcloud_data:
                            # Plotlyã§ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰é¢¨ã®è¡¨ç¤º
                            fig_wc = px.bar(
                                x=list(wordcloud_data.values())[:20],
                                y=list(wordcloud_data.keys())[:20],
                                orientation='h',
                                title="é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰é¢¨ï¼‰",
                                labels={'x': 'å‡ºç¾å›æ•°', 'y': 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'}
                            )
                            fig_wc.update_layout(height=600, yaxis={'categoryorder':'total ascending'})
                            st.plotly_chart(fig_wc, use_container_width=True)
                    
                    elif wc_type == "ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥" and analyzer.clusters:
                        selected_cluster = st.selectbox(
                            "ã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠ", 
                            list(range(len(analyzer.clusters)))
                        )
                        
                        cluster_texts = analyzer.clusters[selected_cluster]['texts']
                        wordcloud_data = analyzer.generate_wordcloud_data(cluster_texts)
                        
                        if wordcloud_data:
                            fig_wc = px.bar(
                                x=list(wordcloud_data.values())[:15],
                                y=list(wordcloud_data.keys())[:15],
                                orientation='h',
                                title=f"ã‚¯ãƒ©ã‚¹ã‚¿ {selected_cluster} ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                                labels={'x': 'å‡ºç¾å›æ•°', 'y': 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'}
                            )
                            fig_wc.update_layout(height=500, yaxis={'categoryorder':'total ascending'})
                            st.plotly_chart(fig_wc, use_container_width=True)
                
                elif viz_option == "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•":
                    st.subheader("ğŸ•¸ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•")
                    
                    if analyzer.network_graph is not None:
                        graph_data = analyzer.network_graph
                        
                        fig_network = go.Figure()
                        
                        # ã‚¨ãƒƒã‚¸è¿½åŠ 
                        fig_network.add_trace(go.Scatter(
                            x=graph_data['edge_x'], y=graph_data['edge_y'],
                            line=dict(width=0.5, color='#888'),
                            hoverinfo='none',
                            mode='lines'
                        ))
                        
                        # ãƒãƒ¼ãƒ‰è¿½åŠ 
                        fig_network.add_trace(go.Scatter(
                            x=graph_data['node_x'], y=graph_data['node_y'],
                            mode='markers',
                            hoverinfo='text',
                            text=graph_data['node_text'],
                            textposition="middle center",
                            marker=dict(
                                size=10,
                                color='lightblue',
                                line=dict(width=2, color='DarkSlateGrey')
                            )
                        ))
                        
                        fig_network.update_layout(
                            title="å›ç­”é–“ã®é–¢é€£æ€§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
                            titlefont_size=16,
                            showlegend=False,
                            hovermode='closest',
                            margin=dict(b=20,l=5,r=5,t=40),
                            annotations=[ dict(
                                text=f"ãƒãƒ¼ãƒ‰æ•°: {graph_data['num_nodes']}, é–¢é€£æ•°: {graph_data['num_edges']}",
                                showarrow=False,
                                xref="paper", yref="paper",
                                x=0.005, y=-0.002 ) ],
                            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                        )
                        
                        st.plotly_chart(fig_network, use_container_width=True)
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ãƒãƒ¼ãƒ‰æ•°", graph_data['num_nodes'])
                        with col2:
                            st.metric("é–¢é€£æ•°", graph_data['num_edges'])
                    else:
                        st.info("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã‚’æœ‰åŠ¹åŒ–ã—ã¦AIåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
                
                elif viz_option == "3Dæ•£å¸ƒå›³":
                    st.subheader("ğŸ“Š 3Dæ•£å¸ƒå›³")
                    
                    if analyzer.embeddings is not None and len(analyzer.embeddings) > 3:
                        # 3D PCA
                        pca_3d = PCA(n_components=3, random_state=42)
                        embeddings_3d = pca_3d.fit_transform(analyzer.embeddings)
                        
                        fig_3d = go.Figure(data=go.Scatter3d(
                            x=embeddings_3d[:, 0],
                            y=embeddings_3d[:, 1], 
                            z=embeddings_3d[:, 2],
                            mode='markers',
                            marker=dict(
                                size=5,
                                color=[f'ã‚¯ãƒ©ã‚¹ã‚¿ {label}' for label in analyzer.cluster_labels] if analyzer.cluster_labels is not None else 'blue',
                                showscale=True
                            ),
                            text=[text[:100] + "..." if len(text) > 100 else text for text in df['text_response']],
                            hovertemplate="<b>%{text}</b><extra></extra>"
                        ))
                        
                        fig_3d.update_layout(
                            title="3D ã‚¯ãƒ©ã‚¹ã‚¿å¯è¦–åŒ– (PCA)",
                            scene=dict(
                                xaxis_title="PC1",
                                yaxis_title="PC2", 
                                zaxis_title="PC3"
                            ),
                            height=600
                        )
                        
                        st.plotly_chart(fig_3d, use_container_width=True)
                    else:
                        st.info("3Då¯è¦–åŒ–ã«ã¯ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
                
                elif viz_option == "æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰":
                    st.subheader("ğŸ“ˆ æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
                    
                    if analyzer.trend_analysis is not None:
                        trend_data = analyzer.trend_analysis
                        
                        # æ—¥åˆ¥å›ç­”æ•°ãƒˆãƒ¬ãƒ³ãƒ‰
                        daily_stats = trend_data['daily_stats']
                        
                        fig_trend = make_subplots(
                            rows=2, cols=1,
                            subplot_titles=('æ—¥åˆ¥å›ç­”æ•°', 'é€±åˆ¥å¹³å‡æ–‡å­—æ•°'),
                            vertical_spacing=0.1
                        )
                        
                        # æ—¥åˆ¥å›ç­”æ•°
                        fig_trend.add_trace(
                            go.Scatter(
                                x=daily_stats.index,
                                y=daily_stats['response_count'],
                                mode='lines+markers',
                                name='å›ç­”æ•°',
                                line=dict(color='blue')
                            ),
                            row=1, col=1
                        )
                        
                        # é€±åˆ¥æ–‡å­—æ•°
                        weekly_data = trend_data['weekly_char_length']
                        fig_trend.add_trace(
                            go.Scatter(
                                x=[str(w) for w in weekly_data.index],
                                y=weekly_data.values,
                                mode='lines+markers',
                                name='å¹³å‡æ–‡å­—æ•°',
                                line=dict(color='red')
                            ),
                            row=2, col=1
                        )
                        
                        fig_trend.update_layout(height=600, showlegend=False)
                        st.plotly_chart(fig_trend, use_container_width=True)
                        
                        # æœˆåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ³ãƒ‰
                        st.subheader("ğŸ“… æœˆåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ³ãƒ‰")
                        monthly_keywords = trend_data['monthly_keywords']
                        
                        if monthly_keywords:
                            months = list(monthly_keywords.keys())
                            selected_month = st.selectbox("æœˆã‚’é¸æŠ", months)
                            
                            if selected_month in monthly_keywords:
                                month_words = monthly_keywords[selected_month]
                                if month_words:
                                    fig_monthly = px.bar(
                                        x=list(month_words.values()),
                                        y=list(month_words.keys()),
                                        orientation='h',
                                        title=f"{selected_month} ã®é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                                        labels={'x': 'å‡ºç¾å›æ•°', 'y': 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'}
                                    )
                                    fig_monthly.update_layout(height=400, yaxis={'categoryorder':'total ascending'})
                                    st.plotly_chart(fig_monthly, use_container_width=True)
                    else:
                        st.info("æ™‚ç³»åˆ—åˆ†æã‚’æœ‰åŠ¹åŒ–ã—ã¦AIåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            else:
                st.info("é«˜åº¦ãªå¯è¦–åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã¾ãšAIåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        # Phase 3: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ»ãƒ¬ãƒãƒ¼ãƒˆã‚¿ãƒ–
        with tab8:
            st.markdown('<h2 class="sub-header">ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ</h2>', unsafe_allow_html=True)
            
            # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰éƒ¨åˆ†
            st.subheader("ğŸ“Š åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
            
            if analyzer.processed_data is not None:
                dashboard_data = analyzer.create_comprehensive_dashboard(df)
                kpis = dashboard_data['kpis']
                
                # KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹
                st.markdown("**ğŸ¯ ä¸»è¦æŒ‡æ¨™ (KPI)**")
                col1, col2, col3, col4, col5 = st.columns(5)
                
                with col1:
                    st.metric(
                        "ç·å›ç­”æ•°", 
                        f"{kpis['total_responses']:,}",
                        help="åˆ†æå¯¾è±¡ã®ç·å›ç­”æ•°"
                    )
                
                with col2:
                    st.metric(
                        "ãƒ¦ãƒ‹ãƒ¼ã‚¯å›ç­”è€…", 
                        f"{kpis['unique_respondents']:,}",
                        help="é‡è¤‡ã‚’é™¤ã„ãŸå›ç­”è€…æ•°"
                    )
                
                with col3:
                    st.metric(
                        "å¹³å‡æ–‡å­—æ•°", 
                        f"{kpis['avg_response_length']:.1f}",
                        help="1å›ç­”ã‚ãŸã‚Šã®å¹³å‡æ–‡å­—æ•°"
                    )
                
                with col4:
                    response_rate = kpis['response_rate'] if kpis['response_rate'] < 10 else 1.0
                    st.metric(
                        "å›ç­”ç‡", 
                        f"{response_rate:.1f}",
                        help="å›ç­”è€…ã‚ãŸã‚Šã®å¹³å‡å›ç­”æ•°"
                    )
                
                with col5:
                    st.metric(
                        "ãƒ‡ãƒ¼ã‚¿å“è³ª", 
                        f"{kpis['data_quality_score']:.1f}%",
                        help="æ¬ æå€¤ã‚’é™¤ã„ãŸãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§"
                    )
                
                # ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã‚µãƒ³ã‚­ãƒ¼å›³
                st.subheader("ğŸ”— ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–¢é€£å›³")
                top_keywords = dashboard_data['top_keywords']
                
                if len(top_keywords) >= 5:
                    # ã‚µãƒ³ã‚­ãƒ¼å›³ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                    keywords = list(top_keywords.keys())[:10]
                    values = list(top_keywords.values())[:10]
                    
                    fig_sankey = go.Figure(data=[go.Sankey(
                        node = dict(
                            pad = 15,
                            thickness = 20,
                            line = dict(color = "black", width = 0.5),
                            label = keywords,
                            color = "blue"
                        ),
                        link = dict(
                            source = [0] * len(keywords[1:]),
                            target = list(range(1, len(keywords))),
                            value = values[1:]
                        )
                    )])
                    
                    fig_sankey.update_layout(title_text="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦ãƒ•ãƒ­ãƒ¼", font_size=10)
                    st.plotly_chart(fig_sankey, use_container_width=True)
                
                # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆéƒ¨åˆ†
                st.subheader("ğŸ“‹ è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
                
                if 'report_insights' in st.session_state:
                    insights = st.session_state.report_insights
                    
                    # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
                    report_tabs = st.tabs(["ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼", "ğŸ’­ ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ", "ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿", "ğŸ“ˆ æ™‚ç³»åˆ—"])
                    
                    with report_tabs[0]:
                        if 'basic_stats' in insights:
                            st.markdown(insights['basic_stats'])
                    
                    with report_tabs[1]:
                        if 'sentiment' in insights:
                            st.markdown(insights['sentiment'])
                    
                    with report_tabs[2]:
                        if 'clusters' in insights:
                            st.markdown(insights['clusters'])
                    
                    with report_tabs[3]:
                        if 'temporal' in insights:
                            st.markdown(insights['temporal'])
                    
                    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                    st.subheader("ğŸ“¥ ãƒ¬ãƒãƒ¼ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        # CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                        csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            label="ğŸ“Š åˆ†æãƒ‡ãƒ¼ã‚¿ (CSV)",
                            data=csv_data,
                            file_name=f"survey_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                    
                    with col2:
                        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                        insights_text = "\n\n".join([f"## {k.replace('_', ' ').title()}\n{v}" for k, v in insights.items()])
                        st.download_button(
                            label="ğŸ’¡ ã‚¤ãƒ³ã‚µã‚¤ãƒˆ (TXT)",
                            data=insights_text,
                            file_name=f"survey_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                            mime="text/plain"
                        )
                    
                    with col3:
                        # JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆåˆ†æçµæœã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼‰
                        export_data = {
                            'kpis': kpis,
                            'top_keywords': top_keywords,
                            'insights': insights,
                            'analysis_timestamp': datetime.now().isoformat()
                        }
                        if analyzer.clusters:
                            export_data['clusters'] = analyzer.clusters
                        
                        json_data = json.dumps(export_data, ensure_ascii=False, indent=2)
                        st.download_button(
                            label="ğŸ“„ åˆ†æçµæœ (JSON)",
                            data=json_data,
                            file_name=f"survey_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                else:
                    st.info("ğŸ‘† å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„")
                
                # ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
                st.subheader("âš™ï¸ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º")
                
                with st.expander("è¡¨ç¤ºè¨­å®š", expanded=False):
                    show_advanced_metrics = st.checkbox("é«˜åº¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º", value=True)
                    chart_theme = st.selectbox("ãƒãƒ£ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ", ["plotly", "plotly_white", "plotly_dark"])
                    update_frequency = st.selectbox("æ›´æ–°é »åº¦", ["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ", "1åˆ†", "5åˆ†", "æ‰‹å‹•"])
                    
                    if show_advanced_metrics:
                        st.info("é«˜åº¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤ºãŒæœ‰åŠ¹ã§ã™")
                        
                        # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                        if analyzer.clusters:
                            st.markdown("**ğŸ§  AIåˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹**")
                            ai_col1, ai_col2, ai_col3 = st.columns(3)
                            
                            with ai_col1:
                                st.metric("æ¤œå‡ºã‚¯ãƒ©ã‚¹ã‚¿æ•°", len(analyzer.clusters))
                            
                            with ai_col2:
                                if analyzer.embeddings is not None:
                                    st.metric("ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ", analyzer.embeddings.shape[1])
                            
                            with ai_col3:
                                cluster_balance = np.std([info['size'] for info in analyzer.clusters.values()])
                                st.metric("ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒ©ãƒ³ã‚¹", f"{cluster_balance:.1f}")
                
                # è‡ªå‹•æ›´æ–°æ©Ÿèƒ½ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
                if st.button("ğŸ”„ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°"):
                    st.rerun()
                
            else:
                st.info("ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ã€ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

else:
    # ãƒ‡ãƒ¼ã‚¿æœªè¨­å®šæ™‚ã®èª¬æ˜
    if not st.session_state.data_uploaded:
        st.info("ğŸ‘† å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVã¾ãŸã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã‚’é–‹å§‹ã—ã¦ãã ã•ã„")
    elif not st.session_state.columns_configured:
        st.info("ğŸ‘† å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦è¨­å®šã‚’å®Œäº†ã—ã¦ãã ã•ã„")
    
    st.markdown("### ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å½¢å¼")
    sample_data = pd.DataFrame({
        'ID': [1, 2, 3], 
        'å›ç­”æ—¥': ['2024-01-15', '2024-01-16', '2024-01-17'],
        'è‡ªç”±å›ç­”': [
            'ã‚µãƒ¼ãƒ“ã‚¹ã«æº€è¶³ã—ã¦ã„ã¾ã™ã€‚ä»Šå¾Œã‚‚åˆ©ç”¨ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚',
            'ã‚‚ã†å°‘ã—ä½¿ã„ã‚„ã™ããªã‚‹ã¨è‰¯ã„ã¨æ€ã„ã¾ã™ã€‚',
            'ã‚µãƒãƒ¼ãƒˆã®å¯¾å¿œãŒæ—©ãã¦åŠ©ã‹ã‚Šã¾ã—ãŸã€‚'
        ],
        'å¹´é½¢': [25, 35, 45],
        'æ€§åˆ¥': ['ç”·æ€§', 'å¥³æ€§', 'ç”·æ€§']
    })
    st.dataframe(sample_data, use_container_width=True)
    
    st.markdown("### ğŸ“ ã‚«ãƒ©ãƒ é¸æŠã«ã¤ã„ã¦")
    st.markdown("""
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«ä»¥ä¸‹ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š
    
    - **å›ç­”è€…ID ã‚«ãƒ©ãƒ **: å„å›ç­”è€…ã‚’è­˜åˆ¥ã™ã‚‹IDï¼ˆä¾‹ï¼šID, user_id, å›ç­”è€…ç•ªå·ï¼‰
    - **å›ç­”æ—¥æ™‚ ã‚«ãƒ©ãƒ **: å›ç­”ã—ãŸæ—¥æ™‚ï¼ˆä¾‹ï¼šå›ç­”æ—¥, created_at, timestampï¼‰  
    - **ãƒ†ã‚­ã‚¹ãƒˆå›ç­” ã‚«ãƒ©ãƒ **: åˆ†æå¯¾è±¡ã®ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹ï¼šè‡ªç”±å›ç­”, comment, feedbackï¼‰
    
    ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚«ãƒ©ãƒ åã¯è‡ªç”±ã«è¨­å®šã§ãã¾ã™ã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«é©åˆ‡ãªã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
    """)
    
    st.markdown("### ğŸš€ æ©Ÿèƒ½ä¸€è¦§")
    st.markdown("""
    **Phase 1 å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½:**
    - âœ… CSV/Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆUTF-8, Shift-JISå¯¾å¿œï¼‰
    - âœ… æŸ”è»Ÿãªã‚«ãƒ©ãƒ é¸æŠæ©Ÿèƒ½
    - âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆé¸æŠã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªã€ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼ï¼‰
    - âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆé€šå¸¸æ¤œç´¢ãƒ»æ­£è¦è¡¨ç¾æ¤œç´¢ï¼‰
    - âœ… é »åº¦åˆ†æï¼ˆå˜èªé »åº¦ã€N-gramåˆ†æï¼‰
    - âœ… åŸºæœ¬å¯è¦–åŒ–ï¼ˆæ£’ã‚°ãƒ©ãƒ•ã€æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ã€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
    - âœ… ç°¡æ˜“ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
    - âœ… æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿æ©Ÿèƒ½
    - âœ… è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿æ©Ÿèƒ½ï¼ˆä»»æ„ã®ã‚«ãƒ©ãƒ ï¼‰
    - âœ… æ¤œç´¢çµæœã®CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    - âœ… åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆè‡ªå‹•ç”Ÿæˆ
    
    **Phase 2 å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½:**
    - âœ… ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆCortex EMBED_TEXT_768ç›¸å½“ã®ãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰
    - âœ… æ„å‘³çš„é¡ä¼¼æ¤œç´¢ï¼ˆVECTOR_COSINE_SIMILARITYç›¸å½“ï¼‰
    - âœ… K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆè‡ªå‹•æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ¨å®šï¼‰
    - âœ… ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æã¨ç‰¹å¾´æŠ½å‡º
    - âœ… 2Då¯è¦–åŒ–ï¼ˆt-SNEã€PCAã€UMAPï¼‰
    - âœ… ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
    - âœ… ä»£è¡¨çš„å›ç­”ã®è‡ªå‹•æŠ½å‡º
    
    **Phase 3 å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½:**
    - âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆï¼ˆå…¨ä½“ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥ï¼‰
    - âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ï¼ˆå›ç­”é–“ã®é–¢é€£æ€§å¯è¦–åŒ–ï¼‰
    - âœ… 3Dæ•£å¸ƒå›³ï¼ˆPCA ã«ã‚ˆã‚‹3æ¬¡å…ƒå¯è¦–åŒ–ï¼‰
    - âœ… æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆæ—¥åˆ¥ã€é€±åˆ¥ã€æœˆåˆ¥ï¼‰
    - âœ… åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆKPIã€ã‚µãƒ³ã‚­ãƒ¼å›³ï¼‰
    - âœ… è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆçµ±è¨ˆã€ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã€ã‚¯ãƒ©ã‚¹ã‚¿ã€æ™‚ç³»åˆ—ï¼‰
    - âœ… å¤šå½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆCSVã€TXTã€JSONï¼‰
    - âœ… ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
    - âœ… ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå¯è¦–åŒ–
    - âœ… AIåˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    
    **ä»Šå¾Œã®æ‹¡å¼µäºˆå®šï¼ˆPhase 3.5ä»¥é™ï¼‰:**
    - ğŸ”„ å®Ÿéš›ã®Snowflake Cortex VECTORé–¢æ•°çµ±åˆ
    - ğŸ”„ PowerPointè‡ªå‹•ç”Ÿæˆ
    - ğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ»ç›£è¦–
    - ğŸ”„ A/Bãƒ†ã‚¹ãƒˆåˆ†ææ©Ÿèƒ½
    - ğŸ”„ äºˆæ¸¬åˆ†æãƒ»ç•°å¸¸æ¤œçŸ¥
    - ğŸ”„ å¤šè¨€èªå¯¾å¿œ
    - ğŸ”„ ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆ
    """)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("**Snowflake Cortex ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«** | Version 3.0.0 (Phase 3)")
st.markdown("*Powered by Streamlit & Snowflake Cortex AI*")